{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikishkaaa/ML-Course/blob/main/%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "outputId": "5a0e827e-b696-4fb8-b1e0-83471c9ee732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "outputId": "60511431-6773-4be6-d8c8-90969fdcc1dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78f7cc168dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "outputId": "052ccf7b-2572-4680-b39d-fcd206ceae03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(data, w, b):\n",
        "  temp = -(data @ w.T +b)\n",
        "  return 1/(1+ torch.exp(-temp))"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model(inputs, w, b)"
      ],
      "metadata": {
        "id": "sh5mPfQuVsQw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "id": "c5TCkk-WZwtl",
        "outputId": "4790133d-b4de-43a3-8a52-7a3befaaa4c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.9871e-31],\n",
              "        [4.7579e-39],\n",
              "        [0.0000e+00],\n",
              "        [2.8692e-36],\n",
              "        [8.0943e-34]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Висновок: всі значення передбачились як позитивний клас, це може бути зумовлено вагами, так як ми їх встановлюємо рандомно"
      ],
      "metadata": {
        "id": "9Ok87NF3YqWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    epsilon = 1e-10 # Small value to avoid problem with log(0)\n",
        "    loss = - (true_labels * torch.log(predicted_probs + epsilon) + (1 - true_labels) * torch.log(1 - predicted_probs + epsilon))\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "ekkP8at2Xa-o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(predicted, targets)\n",
        "print(\"Середнє значення втрат:\", loss)"
      ],
      "metadata": {
        "id": "Q3lC_o4YXglg",
        "outputId": "0e584fe8-4621-4127-c2c2-5e2d2ad36d73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Середнє значення втрат: tensor(13.8155, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Градієнти вагів\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "id": "xRLqmy9neNfy",
        "outputId": "ab61bc79-e162-41e2-d801-f791acbb20c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([[nan, nan, nan]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradients for bias\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "Ge4io4zjeU88",
        "outputId": "008d56e0-f589-4c16-8631-9e1c92ad665a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6213], requires_grad=True)\n",
            "tensor([nan])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(predictions, targets)\n",
        "print(\"Середнє значення втрат:\", loss)\n",
        "\n",
        "loss.backward()\n",
        "print(w)\n",
        "print(w.grad)\n",
        "\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "outputId": "f598971e-7c98-478b-c5d2-3313ce018076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Середнє значення втрат: tensor(0.7051, grad_fn=<MeanBackward0>)\n",
            "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "tensor([[ 8.9583, 22.6147, 12.3318]])\n",
            "tensor([0.0006], requires_grad=True)\n",
            "tensor([0.1206])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тренування протягом 1000 епох\n",
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "Ao3EZcEQf2Sa",
        "outputId": "5e8c3612-ff26-4e65-a01f-2af7bf377393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3348, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(targets), display(preds)"
      ],
      "metadata": {
        "id": "ldLJxx0lf6BF",
        "outputId": "d0ccfc23-6666-4d96-f8ac-e66a3bd3ab43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[0.5773],\n",
              "        [0.6689],\n",
              "        [0.9114],\n",
              "        [0.1603],\n",
              "        [0.8663]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Висновок: досить добре модель робить передбачення. Якщо поставити theselhold =0.6, то всі екзмляри будуть правильно класифіковані"
      ],
      "metadata": {
        "id": "21MhOW9lgKzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "data = TensorDataset(inputs, targets)\n",
        "data[:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "outputId": "62c25e31-faf3-4462-d4c7-32212eefceca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(data, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "outputId": "c5360671-6d16-4146-fea2-3e2b59341e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(3, 3)\n",
        "        self.act1 =  nn.Sigmoid() # Activation function\n",
        "        self.linear2 = nn.Linear(3, 1)\n",
        "\n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.linear2(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogReg()\n",
        "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss_fn = F.binary_cross_entropy"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Модифікована функцію fit для відстеження втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:3]"
      ],
      "metadata": {
        "id": "oMrw7Ugnmg9r",
        "outputId": "24c230a7-d565-4aad-aaf7-8c07a3a55b3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "id": "p1YW8PXDkvKj",
        "outputId": "0c053a29-f7ff-4f16-db38-633f41075ce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.6516\n",
            "Epoch [200/1000], Loss: 0.6513\n",
            "Epoch [300/1000], Loss: 0.6509\n",
            "Epoch [400/1000], Loss: 0.6504\n",
            "Epoch [500/1000], Loss: 0.6500\n",
            "Epoch [600/1000], Loss: 0.6495\n",
            "Epoch [700/1000], Loss: 0.6489\n",
            "Epoch [800/1000], Loss: 0.6483\n",
            "Epoch [900/1000], Loss: 0.6477\n",
            "Epoch [1000/1000], Loss: 0.6470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FWSwt6V6oDxw",
        "outputId": "b1c23ff3-abe7-455e-92e8-30518a99bc50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "id": "nj2bz9RioRQ6",
        "outputId": "c8e6f680-84bc-43b9-8c39-aec1410024c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6493],\n",
              "        [0.6597],\n",
              "        [0.6648],\n",
              "        [0.6150],\n",
              "        [0.6646],\n",
              "        [0.6493],\n",
              "        [0.6597],\n",
              "        [0.6648],\n",
              "        [0.6150],\n",
              "        [0.6646],\n",
              "        [0.6493],\n",
              "        [0.6597],\n",
              "        [0.6648],\n",
              "        [0.6150],\n",
              "        [0.6646]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "id": "OXpyM6VwoTHk",
        "outputId": "780008d4-5c15-4c05-80f4-451ef173efc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Висновок: модель не навчилась"
      ],
      "metadata": {
        "id": "wbuoKjLgo9kE"
      }
    }
  ]
}